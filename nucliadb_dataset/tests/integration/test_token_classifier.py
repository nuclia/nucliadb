# Copyright 2025 Bosutech XXI S.L.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import tempfile
import time

import pyarrow as pa  # type: ignore

from nucliadb_dataset.dataset import NucliaDBDataset, download_all_partitions
from nucliadb_models.resource import KnowledgeBoxObj
from nucliadb_models.text import TextField
from nucliadb_models.utils import FieldIdString
from nucliadb_models.writer import CreateResourcePayload
from nucliadb_protos.dataset_pb2 import TaskType, TrainSet
from nucliadb_sdk.v2.sdk import NucliaDB


def test_datascientist_tokens(sdk: NucliaDB, temp_folder, kb: KnowledgeBoxObj):
    sdk.create_resource(
        kbid=kb.uuid,
        content=CreateResourcePayload(
            texts={FieldIdString("text"): TextField(body="I'm Ramon")},
        ),
    )
    sdk.create_resource(
        kbid=kb.uuid,
        content=CreateResourcePayload(
            texts={FieldIdString("text"): TextField(body="I'm not Ramon")},
        ),
    )

    sdk.create_resource(
        kbid=kb.uuid,
        content=CreateResourcePayload(
            texts={FieldIdString("text"): TextField(body="I'm Aleix")},
        ),
    )

    arrow_filenames = download_all_partitions(
        task="TOKEN_CLASSIFICATION",
        slug=kb.slug,
        sdk=sdk,
        path=temp_folder,
    )

    resource_count = 3
    fields_per_resource = 2  # text and autogenerated title

    for filename in arrow_filenames:
        with pa.memory_map(filename, "rb") as source:
            loaded_array = pa.ipc.open_stream(source).read_all()
            assert len(loaded_array) == resource_count * fields_per_resource


def test_live_token_classification(sdk: NucliaDB, upload_data_token_classification: KnowledgeBoxObj):
    trainset = TrainSet()
    trainset.type = TaskType.TOKEN_CLASSIFICATION
    trainset.filter.labels.append("PERSON")
    trainset.batch_size = 2

    with tempfile.TemporaryDirectory() as tmpdirname:
        fse = NucliaDBDataset(
            sdk=sdk,
            kbid=upload_data_token_classification.uuid,
            trainset=trainset,
            base_path=tmpdirname,
        )
        partitions = fse.get_partitions()
        assert len(partitions) == 1

        # TODO: remove after ticket sc-4488 is fixed
        time.sleep(1)

        filename = fse.read_partition(partitions[0])

        with pa.memory_map(filename, "rb") as source:
            loaded_array = pa.ipc.open_stream(source).read_all()

            resource_count = 3
            fields_per_resource = 2  # text and autogenerated title

            assert len(loaded_array) == resource_count * fields_per_resource


def test_token_classification_with_multiple_labels(
    sdk: NucliaDB, upload_data_token_classification: KnowledgeBoxObj
):
    trainset = TrainSet()
    trainset.type = TaskType.TOKEN_CLASSIFICATION
    trainset.filter.labels.extend(["PERSON", "ANIMAL"])

    with tempfile.TemporaryDirectory() as tmpdirname:
        dataset = NucliaDBDataset(
            sdk=sdk,
            kbid=upload_data_token_classification.uuid,
            trainset=trainset,
            base_path=tmpdirname,
        )
        partitions = dataset.get_partitions()
        assert len(partitions) == 1

        # TODO: remove after ticket sc-4488 is fixed
        time.sleep(1)

        filename = dataset.read_partition(partitions[0])

        with pa.memory_map(filename, "rb") as source:
            loaded_array = pa.ipc.open_stream(source).read_all()

            resource_count = 3
            fields_per_resource = 2  # text and autogenerated title

            assert len(loaded_array) == resource_count * fields_per_resource

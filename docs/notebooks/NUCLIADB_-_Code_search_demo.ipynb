{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff958a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"nucliadb-sdk<=2.42.1\"\n",
    "! pip install nucliadb-dataset\n",
    "! pip install transformers\n",
    "! pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a873d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from nucliadb_sdk import KnowledgeBox, create_knowledge_box, get_or_create\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac219e7",
   "metadata": {},
   "source": [
    "## Setup NucliaDB\n",
    "\n",
    "- Run **NucliaDB** image:\n",
    "```bash\n",
    "docker run -it \\\n",
    "       -e LOG=INFO \\\n",
    "       -p 8080:8080 \\\n",
    "       -p 8060:8060 \\\n",
    "       -p 8040:8040 \\\n",
    "       -v nucliadb-standalone:/data \\\n",
    "       nuclia/nucliadb:latest\n",
    "```\n",
    "- Or install with pip and run:\n",
    "\n",
    "```bash\n",
    "pip install nucliadb\n",
    "nucliadb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1460e846",
   "metadata": {},
   "source": [
    "## Check everything's up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f072459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.get(f\"http://0.0.0.0:8080\")\n",
    "\n",
    "assert response.status_code == 200, \"Ups, it seems something is not properly installed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee6bb4a",
   "metadata": {},
   "source": [
    "## Setup - creating a KB\n",
    "\n",
    "In nucliadb our data containers are called knowledge boxes.\n",
    "\n",
    "To start working, we need to create one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05525c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nucliadb_sdk.knowledgebox.KnowledgeBox at 0x175d900d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_kb = get_or_create(\"my_code_search_kb\")\n",
    "my_kb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb8454b",
   "metadata": {},
   "source": [
    "## Data preparation  - Collection\n",
    "\n",
    "Then we gather the data. \n",
    "\n",
    "In this case we use the inspect library to gather all the functions from our nucliadb_sdk module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21b93092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nucliadb_sdk\n",
    "from inspect import getmembers, isfunction, ismodule,isclass,getsource\n",
    "\n",
    "def get_all_code(target_module):\n",
    "    functions=[]\n",
    "    functions_code=[]\n",
    "    for  module_name, module in getmembers(target_module,ismodule):\n",
    "        if module_name != \"logging\":\n",
    "            functions.extend([(name,fn) for name, fn in getmembers(module, isfunction) if fn.__module__ == module.__name__])\n",
    "            for my_class_name,my_class in [(name,fn) for name, fn in getmembers(module, isclass) if fn.__module__ == module.__name__]:\n",
    "                functions.extend([(name,fn) for name, fn in getmembers(my_class, isfunction) if fn.__module__ == module.__name__ and (\"__\" not in fn.__name__)])\n",
    "    functions_code=[getsource(function) for function_name,function in functions ]\n",
    "    return functions_code\n",
    "my_functions = [i.strip() for i in get_all_code(nucliadb_sdk)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672f1e34",
   "metadata": {},
   "source": [
    "Just a quick check to see how many functions we gathered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ed1be78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b55ccfa",
   "metadata": {},
   "source": [
    "## Data preparation  - Create vectors\n",
    "\n",
    "Once we have all the code, we need to calculate the vectors.\n",
    "In this case we are using:\n",
    "\n",
    "Microsoft's unixcoder-base\n",
    "\n",
    "model_t5 = SentenceTransformer(\"krlvi/sentence-t5-base-nlpl-code_search_net\")\n",
    "\n",
    "model_bert = SentenceTransformer(\"krlvi/sentence-msmarco-bert-base-dot-v5-nlpl-code_search_net\")\n",
    "\n",
    "model_distilroberta = SentenceTransformer(\"flax-sentence-embeddings/st-codesearch-distilroberta-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8a0f310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors_roberta_pool(tokenizer, model, code_list):\n",
    "    encoded_input = tokenizer(list(code_list),padding=True, truncation=True,max_length =1024, return_tensors=\"pt\")\n",
    "    outputs = model(**encoded_input)\n",
    "    return outputs[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b53daa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t5 = SentenceTransformer(\"krlvi/sentence-t5-base-nlpl-code_search_net\")\n",
    "model_bert = SentenceTransformer(\"krlvi/sentence-msmarco-bert-base-dot-v5-nlpl-code_search_net\")\n",
    "model_distilroberta = SentenceTransformer(\"flax-sentence-embeddings/st-codesearch-distilroberta-base\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/unixcoder-base\")\n",
    "\n",
    "model = AutoModel.from_pretrained(\"microsoft/unixcoder-base\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b27bf8",
   "metadata": {},
   "source": [
    "## Upload our Data\n",
    "\n",
    "Now we have the data and we have created the KB (knowledgebox), so we just need to upload our resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "752995f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorset is not created, we will create it for you\n",
      "Vectorset is not created, we will create it for you\n",
      "Vectorset is not created, we will create it for you\n",
      "Vectorset is not created, we will create it for you\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(my_functions)):\n",
    "    label = \"nucliadb_sdk\"\n",
    "    my_kb.upload(\n",
    "        text=my_functions[i],\n",
    "        labels=[f\"code/{label}\"],\n",
    "        vectors={\"unixcoder-meanpooling\": get_vectors_roberta_pool(tokenizer, model,[my_functions[i]]).tolist(),\n",
    "                 \"t5\": model_t5.encode([my_functions[i]])[0].tolist(),\n",
    "                 \"bert\":  model_bert.encode([my_functions[i]])[0].tolist(),\n",
    "                 \"distilroberta\":  model_distilroberta.encode([my_functions[i]])[0].tolist(),\n",
    "                 },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65021855",
   "metadata": {},
   "source": [
    "## Checks I \n",
    "\n",
    "We uploaded only data with one label. \n",
    "\n",
    "But we could have added more if we had code from other modules, or if we wanted to label some other code features\n",
    "\n",
    "Let's check if the numbers agree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31b12fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': LabelSet(count=56, labels={'nucliadb_sdk': 56})}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_kb.get_uploaded_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27bdbf5",
   "metadata": {},
   "source": [
    "## Checks II\n",
    "\n",
    "We can also list all the different sets of vectors we've uploaded and their dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47ba72c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'distilroberta': VectorSet(dimension=768),\n",
       " 'bert': VectorSet(dimension=768),\n",
       " 't5': VectorSet(dimension=768),\n",
       " 'unixcoder-meanpooling': VectorSet(dimension=768)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_kb.list_vectorset().vectorsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bb0d35",
   "metadata": {},
   "source": [
    "## Searches\n",
    "\n",
    "Now let's start with the most interesting part, the searches!\n",
    "\n",
    "We are going to use a small function to iterate over our search results.\n",
    "\n",
    "For legibility reasons I used a simple regex to print only the name of the function,\n",
    "but feel free to modify it if you want the whole code!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d57010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def print_results(model_name, results):\n",
    "    print(f\"\\t***{model_name.upper()} RESULTS***\")\n",
    "    for result in results:\n",
    "        print(\"Function name:\",re.findall('def ([^\\(]+)', result.text)[0], end=\" -- \")\n",
    "        #print(\"Function code:\",'%.300s' %result.text,\"\\n\\t...\")\n",
    "        #print(\"Function labels:\",\" \".join(result.labels))\n",
    "        print(f\"Similarity score: {result.score}\") \n",
    "    print(\"-----------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c53e33",
   "metadata": {},
   "source": [
    "## Text search\n",
    "\n",
    "First we search only in the text fields\n",
    "\n",
    "We will look for `create_resource` and `create a new knowledge box`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c0e7667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t***FULL TEXT SEARCH RESULTS***\n",
      "Function name: create_resource -- Similarity score: 3.6681838035583496\n",
      "Function name: async_create_resource -- Similarity score: 3.6050631999969482\n",
      "Function name: list_resources -- Similarity score: 2.788604259490967\n",
      "Function name: async_list_resources -- Similarity score: 2.716294050216675\n",
      "Function name: async_upload -- Similarity score: 2.551388740539551\n",
      "Function name: upload -- Similarity score: 2.551388740539551\n",
      "Function name: create_resource -- Similarity score: 0.769873857498169\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "results = my_kb.search(text=\"create_resource\")\n",
    "print_results(\"Full text search\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4612ebfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t***FULL TEXT SEARCH RESULTS***\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "results = my_kb.search(text=\"create a new knowledge box\")\n",
    "print_results(\"Full text search\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3611fa3",
   "metadata": {},
   "source": [
    "## Vector search\n",
    "\n",
    "Full text search has its limitations, so let's try our semantic search and compare the results from different models\n",
    "\n",
    "To perform these searches we need to encode our query and pass it to the search function with the `vector` argument.\n",
    "The results will be retrieved in order from more to less similar (based on cosine similarity).\n",
    "Note that you can define a threshold (`min_score`) so that the search will only return results with similarity higher than a certain value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2ad8f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  create a new knowledge box\n",
      "\t***UNICOXDER RESULTS***\n",
      "Function name: get_labels -- Similarity score: 0.41939425468444824\n",
      "Function name: get_or_create -- Similarity score: 0.4122781753540039\n",
      "Function name: create_knowledge_box -- Similarity score: 0.38341182470321655\n",
      "Function name: get_kb -- Similarity score: 0.3585122525691986\n",
      "Function name: list_kbs -- Similarity score: 0.334780216217041\n",
      "Function name: search -- Similarity score: 0.3201015889644623\n",
      "Function name: get_entities -- Similarity score: 0.3016570508480072\n",
      "-----------\n",
      "\t***T5 RESULTS***\n",
      "Function name: create_knowledge_box -- Similarity score: 0.6352006196975708\n",
      "Function name: get_labels -- Similarity score: 0.47330352663993835\n",
      "Function name: get_labels -- Similarity score: 0.4565504193305969\n",
      "Function name: get_kb -- Similarity score: 0.43946319818496704\n",
      "Function name: get_entities -- Similarity score: 0.4362731873989105\n",
      "Function name: async_length -- Similarity score: 0.4227059781551361\n",
      "Function name: length -- Similarity score: 0.421280175447464\n",
      "Function name: async_search -- Similarity score: 0.4205757975578308\n",
      "Function name: search -- Similarity score: 0.3984178900718689\n",
      "Function name: get_or_create -- Similarity score: 0.35420358180999756\n",
      "Function name: process_uploaded_labels_from_search -- Similarity score: 0.3419523239135742\n",
      "Function name: list_kbs -- Similarity score: 0.31234583258628845\n",
      "-----------\n",
      "\t***DISTILROBERTA RESULTS***\n",
      "Function name: create_knowledge_box -- Similarity score: 0.612922191619873\n",
      "Function name: get_or_create -- Similarity score: 0.4198019206523895\n",
      "Function name: create_resource -- Similarity score: 0.36951228976249695\n",
      "Function name: search -- Similarity score: 0.32230624556541443\n",
      "-----------\n",
      "\t***BERT RESULTS***\n",
      "Function name: create_knowledge_box -- Similarity score: 0.612922191619873\n",
      "Function name: get_or_create -- Similarity score: 0.4198019206523895\n",
      "Function name: create_resource -- Similarity score: 0.36951228976249695\n",
      "Function name: search -- Similarity score: 0.32230624556541443\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "query =[\"create a new knowledge box\"]\n",
    "print(\"QUERY: \",query[0])\n",
    "results_unixcoder = my_kb.search(vector=get_vectors_roberta_pool(tokenizer, model, query).tolist(), vectorset=\"unixcoder-meanpooling\", min_score=0.3)\n",
    "print_results(\"Unicoxder\", results_unixcoder)\n",
    "\n",
    "results_t5 = my_kb.search(\n",
    "    vector=model_t5.encode(query)[0], \n",
    "    vectorset=\"t5\", \n",
    "    min_score=0.3)\n",
    "print_results(\"T5\", results_t5)\n",
    "\n",
    "\n",
    "results_roberta = my_kb.search(\n",
    "    vector=model_distilroberta.encode(query)[0], \n",
    "    vectorset=\"distilroberta\", \n",
    "    min_score=0.3)\n",
    "print_results(\"DISTILROBERTA\", results_roberta)\n",
    "\n",
    "\n",
    "results_bert = my_kb.search(\n",
    "    vector=model_bert.encode(query)[0], \n",
    "    vectorset=\"bert\", \n",
    "    min_score=0.3)\n",
    "\n",
    "print_results(\"BERT\", results_roberta)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a50541ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  Upload vectors\n",
      "\t***UNICOXDER RESULTS***\n",
      "Function name: list_vectorset -- Similarity score: 0.33021214604377747\n",
      "-----------\n",
      "\t***T5 RESULTS***\n",
      "Function name: async_upload -- Similarity score: 0.5734764337539673\n",
      "Function name: start_tus_upload -- Similarity score: 0.4033115804195404\n",
      "-----------\n",
      "\t***DISTILROBERTA RESULTS***\n",
      "Function name: async_upload -- Similarity score: 0.5523496866226196\n",
      "Function name: upload -- Similarity score: 0.5517340302467346\n",
      "Function name: patch_tus_upload -- Similarity score: 0.43836045265197754\n",
      "Function name: start_tus_upload -- Similarity score: 0.4324209690093994\n",
      "-----------\n",
      "\t***BERT RESULTS***\n",
      "Function name: async_upload -- Similarity score: 0.5523496866226196\n",
      "Function name: upload -- Similarity score: 0.5517340302467346\n",
      "Function name: patch_tus_upload -- Similarity score: 0.43836045265197754\n",
      "Function name: start_tus_upload -- Similarity score: 0.4324209690093994\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "query =[\"Upload vectors\"]\n",
    "print(\"QUERY: \",query[0])\n",
    "results_unixcoder = my_kb.search(vector=get_vectors_roberta_pool(tokenizer, model, query).tolist(), vectorset=\"unixcoder-meanpooling\", min_score=0.3)\n",
    "print_results(\"Unicoxder\", results_unixcoder)\n",
    "\n",
    "results_t5 = my_kb.search(\n",
    "    vector=model_t5.encode(query)[0], \n",
    "    vectorset=\"t5\", \n",
    "    min_score=0.4)\n",
    "print_results(\"T5\", results_t5)\n",
    "\n",
    "\n",
    "results_roberta = my_kb.search(\n",
    "    vector=model_distilroberta.encode(query)[0].tolist(), \n",
    "    vectorset=\"distilroberta\", \n",
    "    min_score=0.4)\n",
    "print_results(\"DISTILROBERTA\", results_roberta)\n",
    "\n",
    "\n",
    "results_bert = my_kb.search(\n",
    "    vector=model_bert.encode(query)[0].tolist(), \n",
    "    vectorset=\"bert\", \n",
    "    min_score=0.4)\n",
    "\n",
    "print_results(\"BERT\", results_roberta)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d044823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  create labels\n",
      "\t***UNICOXDER RESULTS***\n",
      "Function name: list_vectorset -- Similarity score: 0.33020952343940735\n",
      "-----------\n",
      "\t***T5 RESULTS***\n",
      "Function name: set_labels -- Similarity score: 0.5127634406089783\n",
      "Function name: get_labels -- Similarity score: 0.41345250606536865\n",
      "-----------\n",
      "\t***DISTILROBERTA RESULTS***\n",
      "Function name: set_labels -- Similarity score: 0.6097429394721985\n",
      "Function name: get_labels -- Similarity score: 0.45817020535469055\n",
      "-----------\n",
      "\t***BERT RESULTS***\n",
      "Function name: set_labels -- Similarity score: 0.6097429394721985\n",
      "Function name: get_labels -- Similarity score: 0.45817020535469055\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "query =[\"create labels\"]\n",
    "\n",
    "print(\"QUERY: \",query[0])\n",
    "results_unixcoder = my_kb.search(vector=get_vectors_roberta_pool(tokenizer, model, query).tolist(), vectorset=\"unixcoder-meanpooling\", min_score=0.3)\n",
    "print_results(\"Unicoxder\", results_unixcoder)\n",
    "\n",
    "results_t5 = my_kb.search(\n",
    "    vector=model_t5.encode(query)[0], \n",
    "    vectorset=\"t5\", \n",
    "    min_score=0.4)\n",
    "print_results(\"T5\", results_t5)\n",
    "\n",
    "\n",
    "results_roberta = my_kb.search(\n",
    "    vector=model_distilroberta.encode(query)[0].tolist(), \n",
    "    vectorset=\"distilroberta\", \n",
    "    min_score=0.4)\n",
    "print_results(\"DISTILROBERTA\", results_roberta)\n",
    "\n",
    "\n",
    "results_bert = my_kb.search(\n",
    "    vector=model_bert.encode(query)[0].tolist(), \n",
    "    vectorset=\"bert\", \n",
    "    min_score=0.4)\n",
    "\n",
    "print_results(\"BERT\", results_roberta)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250afca6",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "As we can see the models with better overall results are **T5**,**BERT**, and **DISTILROBERTA**.\n",
    "And as a curiosity, even though the **BERT** and **DISTILROBERTA** were supposed to be different, their results are exactly the same\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
